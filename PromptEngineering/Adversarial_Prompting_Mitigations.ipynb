{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Input Sanitization**"
      ],
      "metadata": {
        "id": "KrAK2fTFOJnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def sanitize_input(user_input):\n",
        "    # Basic checks for harmful or suspicious content\n",
        "    suspicious_patterns = [\n",
        "        r\"(\\b(?:<script|eval|alert|exec|shell|alert|base64|union|select)\\b)\",  # JS/SQL injections\n",
        "        r\"(<[^>]+>)\",  # Strip HTML tags\n",
        "        r\"(\\b(?:drop|delete|--|insert)\\b)\"  # SQL command keywords\n",
        "    ]\n",
        "\n",
        "    # Replace malicious content\n",
        "    for pattern in suspicious_patterns:\n",
        "        user_input = re.sub(pattern, \"\", user_input, flags=re.IGNORECASE)\n",
        "\n",
        "    return user_input.strip()\n",
        "\n",
        "# Example\n",
        "user_input = \"<script>alert('Hacked');</script> DROP table users;\"\n",
        "clean_input = sanitize_input(user_input)\n",
        "print(clean_input)  # Output should be safe input\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk4TqOX7ON19",
        "outputId": "f9bd0084-6480-4e8a-c4bc-7fc15a98ba8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Hacked');  table users;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Safety Filters**"
      ],
      "metadata": {
        "id": "Uf782jO8OSKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize a pre-trained pipeline for toxicity detection using unitary/toxic-bert\n",
        "toxicity_detector = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
        "\n",
        "def safety_filter(text):\n",
        "    \"\"\"\n",
        "    Function to analyze text for toxicity or harmful content using a pre-trained model.\n",
        "    Flags toxic or harmful content and returns a flag message if detected.\n",
        "    \"\"\"\n",
        "    # Run the model to detect toxicity\n",
        "    result = toxicity_detector(text)\n",
        "\n",
        "    # Print the raw model output (just to see everything returned)\n",
        "    print(f\"Raw Model Output: {result}\")\n",
        "\n",
        "    # Extracting the label and score based on the structure\n",
        "    label = result[0].get('label', 'N/A')  # Get label safely, default to 'N/A' if not found\n",
        "    score = result[0].get('score', 0)  # Get score safely, default to 0 if not found\n",
        "\n",
        "    # Print out the exact label and score for further debugging\n",
        "    print(f\"Label: {label}, Score: {score}\")\n",
        "\n",
        "    # Check if the label is toxic and if the score is above the threshold\n",
        "    if 'TOXIC' in label.upper() and score > 0.6:\n",
        "        return f\"Content flagged as toxic (Confidence: {score*100:.2f}%)\"\n",
        "\n",
        "    return \"Content is safe.\"\n",
        "\n",
        "# Example texts\n",
        "safe_text = \"This is a friendly message, have a nice day!\"\n",
        "toxic_text = \"You are worthless and stupid!\"\n",
        "\n",
        "# Test the safety filter\n",
        "print(\"Safe Text Test:\")\n",
        "print(safety_filter(safe_text))  # Should be flagged as safe\n",
        "\n",
        "print(\"\\nToxic Text Test:\")\n",
        "print(safety_filter(toxic_text))  # Should flag the toxic message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZfMBz0BOUqn",
        "outputId": "00614db8-5494-41ca-b68d-281de70bc7b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Safe Text Test:\n",
            "Raw Model Output: [{'label': 'toxic', 'score': 0.0008533769869245589}]\n",
            "Label: toxic, Score: 0.0008533769869245589\n",
            "Content is safe.\n",
            "\n",
            "Toxic Text Test:\n",
            "Raw Model Output: [{'label': 'toxic', 'score': 0.9887049794197083}]\n",
            "Label: toxic, Score: 0.9887049794197083\n",
            "Content flagged as toxic (Confidence: 98.87%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RLHF**"
      ],
      "metadata": {
        "id": "exrWBlQdPEDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For illustration only - implementing RLHF would require a more complex system\n",
        "class ReinforcementLearningWithHumanFeedback:\n",
        "    def __init__(self):\n",
        "        self.model_feedback = {}  # Store human feedback on various responses\n",
        "\n",
        "    def collect_feedback(self, response, human_feedback):\n",
        "        \"\"\"\n",
        "        Store or update feedback on the generated responses.\n",
        "        \"\"\"\n",
        "        self.model_feedback[response] = human_feedback\n",
        "        print(f\"Collected feedback for response: '{response}' -> {human_feedback}\")\n",
        "\n",
        "    def adjust_model(self):\n",
        "        \"\"\"\n",
        "        Adjust the model's behavior based on collected feedback.\n",
        "        This is a simplified version of how RLHF could work.\n",
        "        \"\"\"\n",
        "        for response, feedback in self.model_feedback.items():\n",
        "            if feedback == \"negative\":\n",
        "                self._penalize_response(response)\n",
        "            elif feedback == \"positive\":\n",
        "                self._reward_response(response)\n",
        "\n",
        "    def _penalize_response(self, response):\n",
        "        \"\"\"\n",
        "        Penalize a response based on negative feedback.\n",
        "        Placeholder function to simulate model penalization.\n",
        "        \"\"\"\n",
        "        print(f\"Penalizing response: '{response}' due to negative feedback.\")\n",
        "\n",
        "    def _reward_response(self, response):\n",
        "        \"\"\"\n",
        "        Reward a response based on positive feedback.\n",
        "        Placeholder function to simulate model reward.\n",
        "        \"\"\"\n",
        "        print(f\"Rewarding response: '{response}' due to positive feedback.\")\n",
        "\n",
        "# Example Usage\n",
        "rlhf = ReinforcementLearningWithHumanFeedback()\n",
        "\n",
        "# Collect positive feedback for a response\n",
        "rlhf.collect_feedback(\"You are amazing!\", \"positive\")\n",
        "\n",
        "# Adjust the model based on feedback collected\n",
        "rlhf.adjust_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnJ8VQI2O6tf",
        "outputId": "465340a1-0a8d-4918-f333-6d5507df659b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected feedback for response: 'You are amazing!' -> positive\n",
            "Rewarding response: 'You are amazing!' due to positive feedback.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adversarial Training**"
      ],
      "metadata": {
        "id": "IEek3onQPGHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function for adversarial training\n",
        "def adversarial_training(input_data, model):\n",
        "    \"\"\"\n",
        "    Perform adversarial training by generating adversarial examples\n",
        "    and training the model on them.\n",
        "    \"\"\"\n",
        "    print(f\"Starting adversarial training with input: {input_data}\")\n",
        "    adversarial_examples = generate_adversarial_examples(input_data)\n",
        "\n",
        "    # Train the model on adversarial examples\n",
        "    for example in adversarial_examples:\n",
        "        print(f\"Training on adversarial example: {example}\")\n",
        "        model.train(example)  # This is where you train the model on adversarial examples\n",
        "\n",
        "# Function to generate adversarial examples by slightly perturbing the input data\n",
        "def generate_adversarial_examples(input_data):\n",
        "    \"\"\"\n",
        "    Generate adversarial examples by adding malicious input to the original data.\n",
        "    \"\"\"\n",
        "    adversarial_examples = [input_data + \" malicious_input\"]\n",
        "    print(f\"Generated adversarial example: {adversarial_examples}\")\n",
        "    return adversarial_examples\n",
        "\n",
        "# Dummy model class for illustration\n",
        "class DummyModel:\n",
        "    def train(self, example):\n",
        "        print(f\"Model is training on the example: {example}\")\n",
        "\n",
        "# Example of using adversarial training\n",
        "model = DummyModel()  # Instantiate the dummy model\n",
        "adversarial_training(\"Normal input\", model)  # Perform adversarial training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvRJPaqyPJY1",
        "outputId": "5cd0510c-190f-4211-8ffa-fe9520814b6d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting adversarial training with input: Normal input\n",
            "Generated adversarial example: ['Normal input malicious_input']\n",
            "Training on adversarial example: Normal input malicious_input\n",
            "Model is training on the example: Normal input malicious_input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Content** **Filtering**"
      ],
      "metadata": {
        "id": "p1okTDArPRXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process_content(content):\n",
        "    prohibited_keywords = [\"violence\", \"hate\", \"explicit\"]\n",
        "    for keyword in prohibited_keywords:\n",
        "        if keyword in content.lower():\n",
        "            content = f\"[Content blocked for inappropriate language]\"\n",
        "            break\n",
        "    return content\n",
        "\n",
        "# Example\n",
        "generated_content = \"This is explicit content!\"\n",
        "safe_content = post_process_content(generated_content)\n",
        "print(safe_content)  # Should block inappropriate content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lp96OtfPODE",
        "outputId": "a78faa11-07bd-4bc9-fc11-015db2abd90b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Content blocked for inappropriate language]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contextual Awareness**"
      ],
      "metadata": {
        "id": "Z8Cz8oGXPfoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextualAwareness:\n",
        "    def __init__(self):\n",
        "        self.previous_conversation = []  # List to store previous user inputs\n",
        "\n",
        "    def analyze_context(self, user_input):\n",
        "        \"\"\"\n",
        "        Analyze the user's input and check for suspicious behavior by analyzing the context.\n",
        "        \"\"\"\n",
        "        # Add the current user input to the conversation history\n",
        "        self.previous_conversation.append(user_input)\n",
        "\n",
        "        # Simplified threshold: if there are more than 3 user inputs\n",
        "        if len(self.previous_conversation) > 3:\n",
        "            # Check for manipulation attempts by looking for specific suspicious terms\n",
        "            if \"manipulate\" in \" \".join(self.previous_conversation).lower():\n",
        "                return \"Warning: Suspicious behavior detected.\"\n",
        "\n",
        "        return \"Response is safe.\"\n",
        "\n",
        "# Example Usage\n",
        "context_analyzer = ContextualAwareness()\n",
        "\n",
        "# Test with suspicious input\n",
        "context_check_1 = context_analyzer.analyze_context(\"How can I manipulate the system?\")\n",
        "print(context_check_1)  # Should flag suspicious behavior\n",
        "\n",
        "# Add some more inputs to the conversation and test again\n",
        "context_analyzer.analyze_context(\"Is there a way to cheat the model?\")\n",
        "context_analyzer.analyze_context(\"What if I trick the system?\")\n",
        "context_analyzer.analyze_context(\"Can I manipulate the outcomes?\")\n",
        "context_check_2 = context_analyzer.analyze_context(\"I need to bypass your restrictions.\")\n",
        "\n",
        "# Check if the system flags the suspicious behavior now\n",
        "print(context_check_2)  # Should flag suspicious behavior as more inputs accumulate\n",
        "\n",
        "# Test with a safe response (no suspicious behavior)\n",
        "safe_input = context_analyzer.analyze_context(\"What are the system capabilities?\")\n",
        "print(safe_input)  # Should print \"Response is safe\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtxSP4kgPkQh",
        "outputId": "c992c452-432d-49fa-b485-84747c7bf471"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response is safe.\n",
            "Warning: Suspicious behavior detected.\n",
            "Warning: Suspicious behavior detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**API Ratelimiting**"
      ],
      "metadata": {
        "id": "XnKr6zyLPseV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, limit_per_minute=10):\n",
        "        self.limit_per_minute = limit_per_minute\n",
        "        self.requests = []  # To store the timestamps of requests\n",
        "\n",
        "    def is_rate_limited(self):\n",
        "        \"\"\"\n",
        "        Checks if the rate limit is exceeded.\n",
        "        If so, it returns True; otherwise, returns False and adds the current request time.\n",
        "        \"\"\"\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Remove requests that are older than a minute (60 seconds)\n",
        "        self.requests = [req for req in self.requests if current_time - req < 60]\n",
        "\n",
        "        if len(self.requests) >= self.limit_per_minute:\n",
        "            return True  # Rate limit exceeded\n",
        "\n",
        "        # If rate limit isn't exceeded, log this request\n",
        "        self.requests.append(current_time)\n",
        "        return False\n",
        "\n",
        "    def process_request(self):\n",
        "        \"\"\"\n",
        "        Process the request if the rate limit isn't exceeded.\n",
        "        If rate limit is exceeded, block until the rate limit resets.\n",
        "        \"\"\"\n",
        "        while self.is_rate_limited():\n",
        "            # Manually simulate blocking by printing the message and waiting\n",
        "            print(\"Rate limit exceeded. Blocking request. Waiting...\")\n",
        "            time.sleep(2)  # Sleep for 2 seconds and recheck the rate limit\n",
        "\n",
        "        # Once we are able to process the request (limit isn't exceeded)\n",
        "        print(\"Request processed!\")\n",
        "\n",
        "# Example Usage\n",
        "rate_limiter = RateLimiter()\n",
        "\n",
        "# Simulating multiple requests\n",
        "for i in range(1):  # Trying to make 1 request\n",
        "    rate_limiter.process_request()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so4y1XlgPvDQ",
        "outputId": "6157c1c4-fdae-4789-fcd6-7430872d7511"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request processed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Behavioural Guardrails**"
      ],
      "metadata": {
        "id": "tSLrP0A1P0cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ethical_guardrails(response):\n",
        "    harmful_keywords = [\"violence\", \"hate\"]\n",
        "    for keyword in harmful_keywords:\n",
        "        if keyword in response.lower():\n",
        "            return \"Ethical violation detected. Response blocked.\"\n",
        "    return response\n",
        "\n",
        "# Example\n",
        "response = \"Let's talk about violence.\"\n",
        "safe_response = ethical_guardrails(response)\n",
        "print(safe_response)  # Should block unethical content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcYoAKb3P3ZO",
        "outputId": "b1898de5-f65f-4b34-917f-ba90dd20fa2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ethical violation detected. Response blocked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**User Education**"
      ],
      "metadata": {
        "id": "kjjeOliGP-C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_terms_of_service():\n",
        "    terms = \"\"\"\n",
        "    Terms of Service:\n",
        "    1. Do not use the system for illegal activities.\n",
        "    2. Respect others.\n",
        "    3. Any harmful content will result in a ban.\n",
        "    \"\"\"\n",
        "    print(terms)\n",
        "\n",
        "# Example\n",
        "display_terms_of_service()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twKFrc1nQCIq",
        "outputId": "75646d0d-f859-42a9-dfa3-068c27c9e9de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Terms of Service: \n",
            "    1. Do not use the system for illegal activities.\n",
            "    2. Respect others.\n",
            "    3. Any harmful content will result in a ban.\n",
            "    \n"
          ]
        }
      ]
    }
  ]
}